{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from math import sqrt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import roc_auc_score as AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Finished loading\n"
     ]
    }
   ],
   "source": [
    "# Functions to load the dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def read_data(file_name):\n",
    "#     This function is taken from:\n",
    "#     https://github.com/benhamner/BioResponse/blob/master/Benchmarks/csv_io.py\n",
    "\n",
    "    f = open(file_name)\n",
    "    #ignore header\n",
    "    f.readline()\n",
    "    samples = []\n",
    "    target = []\n",
    "    for line in f:\n",
    "        line = line.strip().split(\",\")\n",
    "        sample = [float(x) for x in line]\n",
    "        samples.append(sample)\n",
    "    return samples\n",
    "\n",
    "def load():\n",
    "\n",
    "# Convenience function to load all data as numpy arrays.\n",
    "\n",
    "    print \"Loading data...\"\n",
    "    filename_train = 'Train_Vectorizer.csv'\n",
    "    filename_test = 'Test_Vectorizer.csv'\n",
    "\n",
    "    train = read_data(\"Train_Vectorizer.csv\")\n",
    "    y_train = np.array([x[0] for x in train])\n",
    "    X_train = np.array([x[1:] for x in train])\n",
    "    X_test = np.array(read_data(\"Test_Vectorizer.csv\"))\n",
    "    print \"Finished loading\"\n",
    "    return X_train, y_train, X_test\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    X_train, y_train, X_test = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Finished loading\n",
      "Creating train and test sets for blending and predicting.\n",
      "0 RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Blending {RandomForests, ExtraTrees, GradientBoosting} + stretching to\n",
    "[0,1]. The blending scheme is related to the idea Jose H. Solorzano\n",
    "presented here:\n",
    "http://www.kaggle.com/c/bioresponse/forums/t/1889/question-about-the-process-of-ensemble-learning/10950#post10950\n",
    "'''You can try this: In one of the 5 folds, train the models, then use\n",
    "the results of the models as 'variables' in logistic regression over\n",
    "the validation data of that fold'''. Or at least this is the\n",
    "implementation of my understanding of that idea :-)\n",
    "The predictions are saved in test.csv. \n",
    "Note: if you increase the number of estimators of the classifiers,\n",
    "e.g. n_estimators=1000, you get a better score/rank on the private\n",
    "test set.\n",
    "Originally published on Github by Emanuele Olivetti.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logloss(attempt, actual, epsilon=1.0e-15):\n",
    "    \"\"\"log loss function\n",
    "    \"\"\"\n",
    "    attempt = np.clip(attempt, epsilon, 1.0-epsilon)\n",
    "    return - np.mean(actual * np.log(attempt) + (1.0 - actual) * np.log(1.0 - attempt))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(0) # seed to shuffle the train set\n",
    "\n",
    "    n_folds = 10\n",
    "    verbose = True\n",
    "    shuffle = False\n",
    "\n",
    "    X, y, X_submission = load()\n",
    "\n",
    "    if shuffle:\n",
    "        idx = np.random.permutation(y.size)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    skf = list(StratifiedKFold(y, n_folds))\n",
    "\n",
    "    clfs = [RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini')]\n",
    "#             RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "#             ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "#             ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "#             GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]\n",
    "\n",
    "    print \"Creating train and test sets for blending and predicting.\"\n",
    "    \n",
    "    dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "    dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs)))\n",
    "    \n",
    "    for j, clf in enumerate(clfs):\n",
    "        print j, clf\n",
    "        dataset_blend_test_j = np.zeros((X_submission.shape[0], len(skf)))\n",
    "        for i, (train, test) in enumerate(skf):\n",
    "            print \"Fold\", i\n",
    "            X_train = X[train]\n",
    "            y_train = y[train]\n",
    "            X_test = X[test]\n",
    "            y_test = y[test]\n",
    "            \n",
    "            \n",
    "            clf.fit(X_train, y_train)\n",
    "            y_submission = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "#     print \"Saving Results.\"\n",
    "#     np.savetxt(fname='test.csv', X=y_submission, fmt='%0.9f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import scipy\n",
    "\n",
    "# Basic CSV IO\n",
    "\n",
    "def read_data(file_name):\n",
    "    f = open(file_name)\n",
    "    #ignore header\n",
    "    f.readline()\n",
    "    samples = []\n",
    "    target = []\n",
    "    for line in f:\n",
    "        line = line.strip().split(\",\")\n",
    "        sample = [float(x) for x in line]\n",
    "        samples.append(sample)\n",
    "    return samples\n",
    "\n",
    "def write_delimited_file(file_path, data,header=None, delimiter=\",\"):\n",
    "    f_out = open(file_path,\"w\")\n",
    "    if header is not None:\n",
    "        f_out.write(delimiter.join(header) + \"\\n\")\n",
    "    for line in data:\n",
    "        if isinstance(line, str):\n",
    "            f_out.write(line + \"\\n\")\n",
    "        else:\n",
    "            f_out.write(delimiter.join(line) + \"\\n\")\n",
    "    f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = read_data(\"Train_Vectorizer.csv\")\n",
    "\n",
    "y = [x[0] for x in train]\n",
    "X = [x[1:] for x in train]\n",
    "\n",
    "test = read_data(\"Train_Vectorizer.csv\")\n",
    "\n",
    "# rf = RandomForestClassifier(n_estimators=100, min_samples_leaf=1)\n",
    "# rf.fit(train[:300], target[:300])\n",
    "\n",
    "#     predicted_probs = rf.predict_proba(test)\n",
    "#     predicted_probs = [\"%f\" % x[1] for x in predicted_probs]\n",
    "#     write_delimited_file(\"rf_benchmark.csv\",\n",
    "#                                 predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.asarray(X)\n",
    "# test = np.array(test)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def run_cv(X,y,clf_class,**kwargs):\n",
    "    # Construct a kfolds object\n",
    "    kf = KFold(len(y),n_folds=5,shuffle=True)\n",
    "    y_pred = y.copy()\n",
    "    \n",
    "    # Iterate through folds\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        # Initialize a classifier with key word arguments\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest:\n",
      "0.339\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "\n",
    "def accuracy(y_true,y_pred):\n",
    "    # NumPy interprets True and False as 1. and 0.\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "# print \"Support vector machines:\"\n",
    "# print \"%.3f\" % accuracy(y, run_cv(X,y,SVC))\n",
    "print \"Random forest:\"\n",
    "print \"%.3f\" % accuracy(y, run_cv(X,y,RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exra Trees Regressor:\n",
      "0.142\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.ensemble import ExtraTreesRegressor as ETR\n",
    "\n",
    "def accuracy(y_true,y_pred):\n",
    "    # NumPy interprets True and False as 1. and 0.\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "# print \"Support vector machines:\"\n",
    "# print \"%.3f\" % accuracy(y, run_cv(X,y,SVC))\n",
    "print \"Exra Trees Regressor:\"\n",
    "print \"%.3f\" % accuracy(y, run_cv(X,y, ETR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_prob_cv(X, y, clf_class, **kwargs):\n",
    "    \n",
    "    kf = KFold(len(y), n_folds=5, shuffle=True)\n",
    "    y_prob = np.zeros((len(y),2))\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        # Predict probabilities, not classes\n",
    "        y_prob[test_index] = clf.predict_proba(X_test)\n",
    "        \n",
    "    return y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.82786590e-01,   2.06205856e-01,   3.72183982e-02,\n",
       "         1.79121330e-01,   0.00000000e+00,   1.97616675e-04,\n",
       "         1.53880316e-04,   1.42607518e-03,   5.16060819e-04,\n",
       "         7.97389748e-04,   1.69738535e-03,   8.01689892e-04,\n",
       "         1.08577640e-04,   1.37059431e-03,   5.92559545e-04,\n",
       "         8.96050786e-05,   6.22377319e-04,   1.58737557e-03,\n",
       "         1.35469276e-03,   4.76317872e-05,   9.42847523e-04,\n",
       "         3.23210704e-03,   2.59393996e-03,   6.86137260e-04,\n",
       "         8.40160552e-04,   5.52449883e-03,   2.34483461e-03,\n",
       "         2.81611381e-04,   1.29975447e-03,   6.03047879e-03,\n",
       "         4.68549659e-04,   1.83753720e-06,   6.25498416e-04,\n",
       "         1.11743497e-03,   1.05039715e-03,   1.74484240e-03,\n",
       "         1.32522512e-03,   8.37641047e-04,   2.05878749e-03,\n",
       "         3.46961004e-03,   3.94630216e-04,   1.16264367e-04,\n",
       "         2.85570128e-03,   1.26299707e-04,   7.29255053e-04,\n",
       "         6.45614855e-04,   1.64138688e-03,   2.71505506e-04,\n",
       "         4.64590654e-03,   8.45332252e-05,   7.37951397e-04,\n",
       "         5.10806191e-04,   1.47525757e-04,   2.35155027e-05,\n",
       "         2.64881885e-04,   2.92913737e-03,   1.50230844e-03,\n",
       "         2.37745977e-03,   4.65763439e-03,   1.40781593e-04,\n",
       "         2.30256525e-04,   1.23739977e-03,   4.93458076e-03,\n",
       "         4.18112325e-04,   1.56535522e-05,   2.02972346e-03,\n",
       "         4.25445104e-05,   6.53715337e-04,   4.20956743e-04,\n",
       "         9.05769212e-04,   2.15931526e-04,   7.25597329e-04,\n",
       "         1.14359920e-03,   1.22237262e-03,   1.13639966e-03,\n",
       "         1.44469113e-03,   1.64163332e-03,   1.23177908e-03,\n",
       "         1.14326724e-03,   1.23103596e-03])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "ETCclf = ExtraTreesClassifier()\n",
    "ETCclf = ETCclf.fit(X, y)\n",
    "\n",
    "ETCclf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(647054, 4)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from treeinterpreter import treeinterpreter as ti\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "model = SelectFromModel(ETCclf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest:\n",
      "0.314\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "\n",
    "def accuracy(y_true,y_pred):\n",
    "    # NumPy interprets True and False as 1. and 0.\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "# print \"Support vector machines:\"\n",
    "# print \"%.3f\" % accuracy(y, run_cv(X,y,SVC))\n",
    "print \"Random forest:\"\n",
    "print \"%.3f\" % accuracy(y, run_cv(X_new,y,RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train MSE, 1089.1301, test MSE: 1077.8334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.40, random_state = 1234)\n",
    "rf = RandomForestRegressor(n_estimators=10, max_depth=1000, min_samples_leaf=1, min_samples_split=2, n_jobs=-1)\n",
    "\n",
    "rf.fit(X, y)\n",
    "rf_test_mse = mean_squared_error(y_test, rf.predict(X_test))\n",
    "rf_train_mse = mean_squared_error(y_train, rf.predict(X_train))\n",
    "\n",
    "print \"train MSE, %.4f, test MSE: %.4f\" % (train_mse, test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# fit estimator\n",
    "est = GradientBoostingClassifier(n_estimators=100, max_depth=3)\n",
    "est.fit(X_train, y_train)\n",
    "\n",
    "# predict class labels\n",
    "pred = est.predict(X_test)\n",
    "\n",
    "# score on test data (accuracy)\n",
    "acc = est.score(X_test, y_test)\n",
    "print 'ACC: %.4f' % acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "# Build a forest and compute the feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=250,\n",
    "                              random_state=1234)\n",
    "\n",
    "forest.fit(X, y)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# # Plot the feature importances of the forest\n",
    "# plt.figure()\n",
    "# plt.title(\"Feature importances\")\n",
    "# plt.bar(range(10), importances[indices][0:10],\n",
    "#        color=\"r\", align=\"center\", yerr=std[indices][0:10])\n",
    "# plt.xticks(range(10), indices)\n",
    "# plt.xlim([-1, 10])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_data(figsize=(8, 5)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gt = plt.plot(x_plot, ground_truth(x_plot), alpha=0.4, label='ground truth')\n",
    "\n",
    "    # plot training and testing data\n",
    "    plt.scatter(X_train, y_train, s=10, alpha=0.4)\n",
    "    plt.scatter(X_test, y_test, s=10, alpha=0.4, color='red')\n",
    "    plt.xlim((0, 10))\n",
    "    plt.ylabel('y')\n",
    "    plt.xlabel('x')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
